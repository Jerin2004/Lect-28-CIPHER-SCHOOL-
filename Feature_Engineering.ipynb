{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jerin2004/Lect-28-CIPHER-SCHOOL-/blob/main/Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0ed3986",
      "metadata": {
        "id": "b0ed3986"
      },
      "source": [
        "# 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fca13fa",
      "metadata": {
        "id": "0fca13fa"
      },
      "source": [
        "### Definition\n",
        "Handling missing values is crucial in data preprocessing. Common methods include imputation (replacing missing values with the mean, median, mode, or a specific value) and removing rows or columns with missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73cf0dfd",
      "metadata": {
        "id": "73cf0dfd"
      },
      "source": [
        "### Example Table Data\n",
        "\n",
        "| Feature1 | Feature2 | Feature3 |\n",
        "|----------|----------|----------|\n",
        "| 1.0      | 2.0      | NaN      |\n",
        "| 2.0      | NaN      | 3.0      |\n",
        "| NaN      | 4.0      | 3.5      |\n",
        "| 4.0      | 5.0      | 4.0      |\n",
        "| 5.0      | NaN      | 4.5      |\n",
        "\n",
        "In this table, some values are missing and need to be handled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76dbc47d",
      "metadata": {
        "id": "76dbc47d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': [1.0, 2.0, None, 4.0, 5.0],\n",
        "    'Feature2': [2.0, None, 4.0, 5.0, None],\n",
        "    'Feature3': [None, 3.0, 3.5, 4.0, 4.5]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Handling missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "print(\"After Imputation:\\n\", df_imputed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad4bc47",
      "metadata": {
        "id": "9ad4bc47"
      },
      "source": [
        "# 2. Encoding Categorical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07be05bc",
      "metadata": {
        "id": "07be05bc"
      },
      "source": [
        "### Definition\n",
        "Encoding categorical variables involves converting categorical data into a numerical format that can be used by machine learning algorithms. Common methods include one-hot encoding and label encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c676384b",
      "metadata": {
        "id": "c676384b"
      },
      "source": [
        "### Example Table Data\n",
        "\n",
        "| Color |\n",
        "|-------|\n",
        "| Red   |\n",
        "| Blue  |\n",
        "| Green |\n",
        "| Blue  |\n",
        "| Red   |\n",
        "\n",
        "In this table, 'Color' is a categorical variable that needs to be encoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "170d4a94",
      "metadata": {
        "id": "170d4a94"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Encoding categorical variables\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_categories = encoder.fit_transform(df[['Color']])\n",
        "df_encoded = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(['Color']))\n",
        "df = pd.concat([df, df_encoded], axis=1).drop('Color', axis=1)\n",
        "print(\"After One-Hot Encoding:\\n\", df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b25333",
      "metadata": {
        "id": "42b25333"
      },
      "source": [
        "# 3. Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa64614",
      "metadata": {
        "id": "cfa64614"
      },
      "source": [
        "### Definition\n",
        "Feature scaling involves normalizing or standardizing features so that they have a similar scale. Common methods include min-max scaling and standardization (z-score normalization)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc28b62",
      "metadata": {
        "id": "9fc28b62"
      },
      "source": [
        "### Example Table Data\n",
        "\n",
        "| Feature1 | Feature2 |\n",
        "|----------|----------|\n",
        "| 10       | 100      |\n",
        "| 20       | 200      |\n",
        "| 30       | 300      |\n",
        "| 40       | 400      |\n",
        "| 50       | 500      |\n",
        "\n",
        "In this table, 'Feature1' and 'Feature2' have different scales and need to be normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff48e3f",
      "metadata": {
        "id": "2ff48e3f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': [10, 20, 30, 40, 50],\n",
        "    'Feature2': [100, 200, 300, 400, 500]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "print(\"After Min-Max Scaling:\\n\", df_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8506a806",
      "metadata": {
        "id": "8506a806"
      },
      "source": [
        "# 4. Feature Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2d060b3",
      "metadata": {
        "id": "e2d060b3"
      },
      "source": [
        "### Definition\n",
        "Feature creation involves generating new features from existing ones to improve the predictive power of machine learning models. Common methods include polynomial features and interaction terms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172e6b3a",
      "metadata": {
        "id": "172e6b3a"
      },
      "source": [
        "### Example Table Data\n",
        "\n",
        "| Feature1 | Feature2 |\n",
        "|----------|----------|\n",
        "| 1        | 2        |\n",
        "| 2        | 3        |\n",
        "| 3        | 4        |\n",
        "| 4        | 5        |\n",
        "| 5        | 6        |\n",
        "\n",
        "In this table, new features can be created from 'Feature1' and 'Feature2'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572546f7",
      "metadata": {
        "id": "572546f7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': [1, 2, 3, 4, 5],\n",
        "    'Feature2': [2, 3, 4, 5, 6]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Feature creation\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "poly_features = poly.fit_transform(df)\n",
        "df_poly = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['Feature1', 'Feature2']))\n",
        "print(\"After Creating Polynomial Features:\\n\", df_poly)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}